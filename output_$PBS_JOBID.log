=== Job Started at Sat Aug 16 05:25:31 IST 2025 ===
Job ID: 3741.master
Working Directory: /home/ece_23104085/llm_inference/cuda/gpt2-small-inference
Node: master
=== MIG Setup ===
CUDA_VISIBLE_DEVICES set to: MIG-4deb4e3f-78a4-599a-8d06-8095658c51e8
=== GPU Information ===
GPU 0: NVIDIA H100 PCIe (UUID: GPU-a2ee61d0-205e-2913-d3f8-10c7e81b6304)
  MIG 1g.20gb     Device  0: (UUID: MIG-dce63a51-39c8-5042-b58b-cc944787a482)
  MIG 1g.20gb     Device  1: (UUID: MIG-4deb4e3f-78a4-599a-8d06-8095658c51e8)
  MIG 1g.10gb     Device  2: (UUID: MIG-4d974ea0-ca11-51ef-b078-651c6ea48fac)
  MIG 1g.10gb     Device  3: (UUID: MIG-52699730-a5ff-5d22-9b13-86afc0c39582)
  MIG 1g.10gb     Device  4: (UUID: MIG-86d64fc3-d73b-52bc-9abc-4512bba31293)
  MIG 1g.10gb     Device  5: (UUID: MIG-89c70054-ca2c-5bc6-a9c9-4117fd36347d)
GPU 1: NVIDIA H100 PCIe (UUID: GPU-63af0eea-3926-506e-5128-9a7f8af88ea6)
  MIG 1g.20gb     Device  0: (UUID: MIG-5f487e61-4c41-5418-a42e-59fd29980e2e)
  MIG 1g.20gb     Device  1: (UUID: MIG-cdbe7a23-80f4-5d8c-9747-08541fd90c03)
  MIG 1g.10gb     Device  2: (UUID: MIG-6ac01fe3-31c2-55ae-82fb-2c0720b9adb6)
  MIG 1g.10gb     Device  3: (UUID: MIG-dd5b1cb4-877d-505d-9408-268d763c213c)
  MIG 1g.10gb     Device  4: (UUID: MIG-19dafdf3-91b7-5ec9-b513-4bb5951abc02)
  MIG 1g.10gb     Device  5: (UUID: MIG-0ee28348-8598-5778-bbe2-3223712af17c)
name, memory.total [MiB], memory.used [MiB], memory.free [MiB]
NVIDIA H100 PCIe, 81559 MiB, 87 MiB, 81003 MiB
NVIDIA H100 PCIe, 81559 MiB, 241 MiB, 80850 MiB

=== CUDA Environment ===
/usr/local/cuda-11.8/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Sep_21_10:33:58_PDT_2022
Cuda compilation tools, release 11.8, V11.8.89
Build cuda_11.8.r11.8/compiler.31833905_0

=== Checking Inference Engine ===
Inference engine found.

=== Checking Model Weights ===
Model weights file found.

=== Starting GPT-2 Inference Engine at Sat Aug 16 05:25:31 IST 2025 ===
Running: ./inference_engine
==================================================
Weights loaded to GPU successfully.
Allocating 208 MB of GPU memory...
Starting inference...
=== Generation Step 1 ===
1. Embedding lookup...
   Embedding completed.
2. Processing transformer layer 0...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -0.691133
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: -0.799450
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -0.255816
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 0 completed.
2. Processing transformer layer 1...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 15.968766
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: -0.746378
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.392745
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 1 completed.
2. Processing transformer layer 2...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -8.130253
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: -0.586873
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -2.208058
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 2 completed.
2. Processing transformer layer 3...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 3.039946
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: 0.382280
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 2.274701
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 3 completed.
2. Processing transformer layer 4...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 37.790501
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: -0.324519
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -2.652895
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 4 completed.
2. Processing transformer layer 5...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 9.554612
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: 0.199965
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -1.565385
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 5 completed.
2. Processing transformer layer 6...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -19.435310
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: 0.915948
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 2.829975
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 6 completed.
2. Processing transformer layer 7...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -26.977974
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: 2.023066
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 3.202271
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 7 completed.
2. Processing transformer layer 8...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 14.020242
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: 1.726371
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 2.990193
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 8 completed.
2. Processing transformer layer 9...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -11.702173
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: -0.917606
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -5.094780
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 9 completed.
2. Processing transformer layer 10...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 3.335521
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: -2.245716
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -7.741211
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 10 completed.
2. Processing transformer layer 11...
DEBUG: attention_forward called with B=1, T=1, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 2304 elements
  - d_att: 12 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using simplified T=1 attention computation
DEBUG: T=1 attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -9.053349
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=1
DEBUG: total_size=12
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using simplified T=1 attention-value computation
DEBUG: Head 0 attention weight: 1.000000
DEBUG: Head 1 attention weight: 1.000000
DEBUG: Head 2 attention weight: 1.000000
DEBUG: Head 3 attention weight: 1.000000
DEBUG: Head 4 attention weight: 1.000000
DEBUG: Head 5 attention weight: 1.000000
DEBUG: Head 6 attention weight: 1.000000
DEBUG: Head 7 attention weight: 1.000000
DEBUG: Head 8 attention weight: 1.000000
DEBUG: Head 9 attention weight: 1.000000
DEBUG: Head 10 attention weight: 1.000000
DEBUG: Head 11 attention weight: 1.000000
DEBUG: T=1 attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=1
DEBUG: d_y is readable, first value: 0.368680
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 17.514997
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 11 completed.
3. Final LayerNorm...
   Final LayerNorm completed.
4. Computing logits...
   Logits computation completed.
5. Copying logits to host for sampling...
6. Performing top-k sampling...
Generated token: 41683 (Step 1 completed)
=== Generation Step 2 ===
1. Embedding lookup...
   Embedding completed.
2. Processing transformer layer 0...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -0.691133
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: -0.799451
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.554342
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 0 completed.
2. Processing transformer layer 1...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -2.573526
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: -0.414788
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 0.648964
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 1 completed.
2. Processing transformer layer 2...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 8.638193
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: 1.036184
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.280860
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 2 completed.
2. Processing transformer layer 3...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 8.261260
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: 0.026087
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -4.617888
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 3 completed.
2. Processing transformer layer 4...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -13.565717
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: 1.063375
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -5.654229
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 4 completed.
2. Processing transformer layer 5...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 3.613040
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: -1.516326
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 4.833667
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 5 completed.
2. Processing transformer layer 6...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 3.544105
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: -0.050235
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -1.970382
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 6 completed.
2. Processing transformer layer 7...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 3.948442
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: 3.927803
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 0.621938
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 7 completed.
2. Processing transformer layer 8...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -12.524088
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: 0.937842
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -10.219408
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 8 completed.
2. Processing transformer layer 9...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -1.794633
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: -0.453466
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 0.098116
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 9 completed.
2. Processing transformer layer 10...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -7.777703
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: 0.401417
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -4.418845
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 10 completed.
2. Processing transformer layer 11...
DEBUG: attention_forward called with B=1, T=2, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 4608 elements
  - d_att: 48 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=2
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 22.303045
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=2
DEBUG: total_size=48
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=2
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=2
DEBUG: d_y is readable, first value: 2.301666
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 8.912966
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 11 completed.
3. Final LayerNorm...
   Final LayerNorm completed.
4. Computing logits...
   Logits computation completed.
5. Copying logits to host for sampling...
6. Performing top-k sampling...
Generated token: 2877 (Step 2 completed)
=== Generation Step 3 ===
1. Embedding lookup...
   Embedding completed.
2. Processing transformer layer 0...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -0.691133
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: -0.799451
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.757454
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 0 completed.
2. Processing transformer layer 1...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 3.372220
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: 1.381960
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -0.353793
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 1 completed.
2. Processing transformer layer 2...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 26.807901
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: 0.848260
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.184777
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 2 completed.
2. Processing transformer layer 3...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 16.631357
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: -0.437047
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -1.753412
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 3 completed.
2. Processing transformer layer 4...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 5.277888
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: -0.805361
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -1.508523
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 4 completed.
2. Processing transformer layer 5...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -5.772421
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: -1.948934
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.789689
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 5 completed.
2. Processing transformer layer 6...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -4.738743
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: 0.222667
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 3.296313
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 6 completed.
2. Processing transformer layer 7...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 20.976753
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: 0.525627
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -2.812114
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 7 completed.
2. Processing transformer layer 8...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -21.421230
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: -1.894039
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -5.338656
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 8 completed.
2. Processing transformer layer 9...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 6.165153
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: 1.938691
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -0.120773
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 9 completed.
2. Processing transformer layer 10...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -0.386286
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: 0.061354
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -4.925163
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 10 completed.
2. Processing transformer layer 11...
DEBUG: attention_forward called with B=1, T=3, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 6912 elements
  - d_att: 108 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=3
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -62.148525
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=3
DEBUG: total_size=108
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=3
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=3
DEBUG: d_y is readable, first value: 3.029267
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 6.277204
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 11 completed.
3. Final LayerNorm...
   Final LayerNorm completed.
4. Computing logits...
   Logits computation completed.
5. Copying logits to host for sampling...
6. Performing top-k sampling...
Generated token: 21959 (Step 3 completed)
=== Generation Step 4 ===
1. Embedding lookup...
   Embedding completed.
2. Processing transformer layer 0...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -0.691133
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: -0.799451
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.098337
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 0 completed.
2. Processing transformer layer 1...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 1.056247
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: 0.666054
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 0.527542
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 1 completed.
2. Processing transformer layer 2...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -19.306721
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: -0.650842
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -0.031809
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 2 completed.
2. Processing transformer layer 3...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 7.322470
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: -2.250673
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -0.968891
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 3 completed.
2. Processing transformer layer 4...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -1.380109
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: -0.795213
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -2.027761
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 4 completed.
2. Processing transformer layer 5...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 1.580455
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: -2.610217
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -3.504544
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 5 completed.
2. Processing transformer layer 6...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 26.029005
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: 0.489669
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 3.327183
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 6 completed.
2. Processing transformer layer 7...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -22.719818
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: 2.910873
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 0.956661
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 7 completed.
2. Processing transformer layer 8...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -23.153715
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: 0.681519
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -3.303677
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 8 completed.
2. Processing transformer layer 9...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 3.879867
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: -0.189807
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -8.421197
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 9 completed.
2. Processing transformer layer 10...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -0.804680
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: 1.714127
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -9.057988
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 10 completed.
2. Processing transformer layer 11...
DEBUG: attention_forward called with B=1, T=4, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 9216 elements
  - d_att: 192 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=4
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -34.867180
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=4
DEBUG: total_size=192
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=4
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=4
DEBUG: d_y is readable, first value: 0.501355
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -4.205421
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 11 completed.
3. Final LayerNorm...
   Final LayerNorm completed.
4. Computing logits...
   Logits computation completed.
5. Copying logits to host for sampling...
6. Performing top-k sampling...
Generated token: 12368 (Step 4 completed)
=== Generation Step 5 ===
1. Embedding lookup...
   Embedding completed.
2. Processing transformer layer 0...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -0.691133
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: -0.799451
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.961600
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 0 completed.
2. Processing transformer layer 1...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 3.262022
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: 0.730083
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 3.987837
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 1 completed.
2. Processing transformer layer 2...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 7.961770
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: 1.156225
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -2.487922
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 2 completed.
2. Processing transformer layer 3...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -8.574571
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: -1.164966
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -1.002896
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 3 completed.
2. Processing transformer layer 4...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 5.640096
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: 1.647940
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 6.024440
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 4 completed.
2. Processing transformer layer 5...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 6.538395
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: -2.157877
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -2.288315
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 5 completed.
2. Processing transformer layer 6...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -15.235176
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: 0.183318
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 0.248582
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 6 completed.
2. Processing transformer layer 7...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -12.585757
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: -0.663398
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 0.110130
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 7 completed.
2. Processing transformer layer 8...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -10.725659
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: -1.600913
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: -5.283829
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 8 completed.
2. Processing transformer layer 9...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 3.673486
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: 2.584283
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 8.705536
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 9 completed.
2. Processing transformer layer 10...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: -4.936624
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: -3.110219
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.325329
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 10 completed.
2. Processing transformer layer 11...
DEBUG: attention_forward called with B=1, T=5, C=768, NH=12
DEBUG: Allocating attention memory...
  - d_qkv: 11520 elements
  - d_att: 300 elements
DEBUG: Memory allocation successful
DEBUG: Starting QKV projection...
DEBUG: QKV projection successful
DEBUG: QKV bias addition successful
DEBUG: Splitting Q, K, V...
DEBUG: Q, K, V split successful
DEBUG: Starting attention computation...
DEBUG: Using individual attention computation for T=5
DEBUG: Individual attention computation completed
DEBUG: Testing attention matrix readability...
DEBUG: Attention matrix is readable, first value: 66.663033
DEBUG: About to call attention_softmax_forward...
DEBUG: attention_softmax called with B=1, NH=12, T=5
DEBUG: total_size=300
DEBUG: attention_softmax completed successfully
DEBUG: attention_softmax_forward completed
DEBUG: Starting attention-value multiplication...
DEBUG: Pointer arrays set for att@v multiplication
DEBUG: Using individual attention-value computation for T=5
DEBUG: Individual attention-value computation completed
DEBUG: Starting final projection...
DEBUG: Matrix dimensions - C=768, B*T=5
DEBUG: d_y is readable, first value: 1.447721
DEBUG: Final projection successful
DEBUG: Output is readable after projection, first value: 1.397948
DEBUG: Adding bias...
DEBUG: Bias addition completed
DEBUG: Starting memory cleanup...
DEBUG: Freeing d_qkv...
DEBUG: Freeing d_q...
DEBUG: Freeing d_k...
DEBUG: Freeing d_v...
DEBUG: Freeing d_att...
DEBUG: Freeing d_att_softmax...
DEBUG: Freeing d_y...
DEBUG: Memory cleanup completed successfully
   Layer 11 completed.
3. Final LayerNorm...
   Final LayerNorm completed.
4. Computing logits...
   Logits computation completed.
5. Copying logits to host for sampling...
6. Performing top-k sampling...
Generated token: 46234 (Step 5 completed)
Generated 5 tokens in 118 ms
Tokens/sec: 42.3729
==================================================
=== Inference Engine Completed at Sat Aug 16 05:25:32 IST 2025 with exit code: 0 ===
=== Final GPU Status ===
memory.used [MiB], memory.free [MiB]
87 MiB, 81003 MiB
241 MiB, 80850 MiB

=== Job Finished at Sat Aug 16 05:25:33 IST 2025 ===
Logs saved to:
  Output: ./logs/3741.master.log
  Errors: ./logs/3741.master_error.log
